{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc90b26f-d08e-426c-98dd-a454460b247a",
   "metadata": {},
   "source": [
    "# **Chapter 8 - Dimensionality Reduction**\n",
    "Many Machine Learning Problems Involve thousands or even hundreds of features to Train the Models which In turn gives birth to **the curse of dimensionality**(Explained Later in the Chapter) and as a solution to this Problem comes Dimensionality Reduction here are two of the advantages of using Dimensionality Reduction :-\n",
    "- Speed up the Process of Training.\n",
    "- Helpful in Data Visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2560910e-6e6c-43a7-a3ce-603e1f6e0d77",
   "metadata": {},
   "source": [
    "# The Curse of Dimensionality\n",
    "- Sometimes Our Machine Learning Model gets confused with the Noise of the Dataset.\n",
    "    - Most of the times due to high number of features.\n",
    "- High Numbers of features in a dataset results in slow speed of Training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8749b2-bb4f-4b6b-a8b5-c9a2093290c0",
   "metadata": {},
   "source": [
    "# Main Approaches of Dimensionality Reduction\n",
    "There are two main Approaches of Dimensionality Reduction in Machine Learning :-\n",
    "\n",
    "## Projection\n",
    "In Most Real World Problems the the Dataset is not Uniformly Distrubuted Throughout all Dimensions. Many Features are almost Constant.\n",
    "So in the Projection We Find out the Plane in which most of the data points lie and then Project the Whole Dataset on that specific Plane.\n",
    "However somtimes we cannot find a plane on the Dataset Like in the famous Swiss Roll Dataset in Which You have to unroll the dataset to get accurate representation.\n",
    "\n",
    "## Manifold Learning\n",
    "Manifold Learning is a Dimensionality Reduction Algorithm which Tries to find Familiar Shapes or Structures inside the Dataset and then Projects the Dataset Unto a Lower Dimensional Plane hence Reducing its Dimension.\n",
    "- i.e > Unrolling the Swiss Roll Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d77bc5-f59f-436b-8009-40324831a484",
   "metadata": {},
   "source": [
    "# PCA(Principal Component Analysis)\n",
    "- This is the Most Popular Dimensionality Redution Algorithm.\n",
    "- First It Identifies the Hyperplane which Lies closest to the Data.\n",
    "- Then It Projects the Data Onto that Hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42866d89-4ab5-4610-8a4e-77fc025e86b9",
   "metadata": {},
   "source": [
    "## Preserving the Variance\n",
    "- Before You Project the Data onto the Hyperplane You Need to Select the Right Hyperplane.\n",
    "- The Best Way to Select the Right Hyperplane is to choose the Hyperplane which Preserves the Maximum Variance.\n",
    "    - Because it will in turn preserve the Data Loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999be24b-9844-4673-b202-562727fe41ac",
   "metadata": {},
   "source": [
    "## Principal Component\n",
    "- PCA Identifies the axis that accounts the largest amount of Variance in the training Set.\n",
    "- The $i^{th}$ axis is also called the $i^{th}$ *principal component*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53ce0eb-9b7f-4136-9a9a-a937208ba259",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
