{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc90b26f-d08e-426c-98dd-a454460b247a",
   "metadata": {},
   "source": [
    "# **Chapter 8 - Dimensionality Reduction**\n",
    "Many Machine Learning Problems Involve thousands or even hundreds of features to Train the Models which In turn gives birth to **the curse of dimensionality**(Explained Later in the Chapter) and as a solution to this Problem comes Dimensionality Reduction here are two of the advantages of using Dimensionality Reduction :-\n",
    "- Speed up the Process of Training.\n",
    "- Helpful in Data Visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2560910e-6e6c-43a7-a3ce-603e1f6e0d77",
   "metadata": {},
   "source": [
    "# The Curse of Dimensionality\n",
    "- Sometimes Our Machine Learning Model gets confused with the Noise of the Dataset.\n",
    "    - Most of the times due to high number of features.\n",
    "- High Numbers of features in a dataset results in slow speed of Training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8749b2-bb4f-4b6b-a8b5-c9a2093290c0",
   "metadata": {},
   "source": [
    "# Main Approaches of Dimensionality Reduction\n",
    "There are two main Approaches of Dimensionality Reduction in Machine Learning :-\n",
    "\n",
    "## Projection\n",
    "In Most Real World Problems the the Dataset is not Uniformly Distrubuted Throughout all Dimensions. Many Features are almost Constant.\n",
    "So in the Projection We Find out the Plane in which most of the data points lie and then Project the Whole Dataset on that specific Plane.\n",
    "However somtimes we cannot find a plane on the Dataset Like in the famous Swiss Roll Dataset in Which You have to unroll the dataset to get accurate representation.\n",
    "\n",
    "## Manifold Learning\n",
    "Manifold Learning is a Dimensionality Reduction Algorithm which Tries to find Familiar Shapes or Structures inside the Dataset and then Projects the Dataset Unto a Lower Dimensional Plane hence Reducing its Dimension.\n",
    "- i.e > Unrolling the Swiss Roll Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d77bc5-f59f-436b-8009-40324831a484",
   "metadata": {},
   "source": [
    "# PCA(Principal Component Analysis)\n",
    "- This is the Most Popular Dimensionality Redution Algorithm.\n",
    "- First It Identifies the Hyperplane which Lies closest to the Data.\n",
    "- Then It Projects the Data Onto that Hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42866d89-4ab5-4610-8a4e-77fc025e86b9",
   "metadata": {},
   "source": [
    "## Preserving the Variance\n",
    "- Before You Project the Data onto the Hyperplane You Need to Select the Right Hyperplane.\n",
    "- The Best Way to Select the Right Hyperplane is to choose the Hyperplane which Preserves the Maximum Variance.\n",
    "    - Because it will in turn preserve the Data Loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999be24b-9844-4673-b202-562727fe41ac",
   "metadata": {},
   "source": [
    "## Principal Component\n",
    "- PCA Identifies the axis that accounts the largest amount of Variance in the training Set.\n",
    "- The $i^{th}$ axis is also called the $i^{th}$ *principal component*.\n",
    "- We find the principal component for the dataset with help of a standard matrix factorization called **Singular Value Decomposition(SVD)** here is an implementation of this function with python code :-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6842db8-46db-44f8-9411-8b7e4270a867",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c33a4d7-c042-45d8-b17b-cd9bf3f3c5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_centered = X - X.mean(axis=0)\n",
    "U, s, Vt = np.linalg.svd(X_centered)\n",
    "c1 = Vt.T[:, 0]\n",
    "c2 = Vt.T[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60efb71-223b-4004-89a3-2d7dacc64667",
   "metadata": {},
   "source": [
    "**V** contains all of the Pricipal Components that we are looking for :-\n",
    "$$\n",
    "V = (c_1, c_2, ..., c_n)\n",
    "$$\n",
    "- here vectors from $c_1$ to $c_n$ are the Pricipal Components from which main Pricipal Componenet will be choosed on the basis of Preserving Variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95b0956-6a5c-426f-b56b-48f8cf097560",
   "metadata": {},
   "source": [
    "## Projecting Down to d Dimensions\n",
    "Now that we have Identified our Pricipal Component we can obtain the reduced data by Prjecting it onto the Pricipal component of our choice.\n",
    "- To Project the training set onto the hyperplane and obtain a reduced dataset $X_{d-proj}$ we have to compute a matrix multiplication.\n",
    "\n",
    "$$\n",
    "X_{d-proj} = XW_d\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "682c5469-a8bc-490a-ab87-bb2708fc9ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = Vt.T[:, :2]\n",
    "X2D = X_centered.dot(W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65036e53-534f-4883-b26e-07f9257de900",
   "metadata": {},
   "source": [
    "## Using Scikit-Learn\n",
    "Like all the other Techniques and other things *Scikit-Learn* also Provides an implementation of **PCA** :-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8843c81-94d9-4e9f-bbf2-12f5aafd073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X2D = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e558190-471d-4cb1-ab94-9693488007a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.81968748, 0.18031252])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2185ef-c0eb-4ee1-b7a4-eb63b6e2a319",
   "metadata": {},
   "source": [
    "This tells YOu that the VAriance in the first PC was 81% and in the Second PC it was 18% which means that we have Preserved 81% of Our Data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6272a09-4bf6-40da-80c5-9c5f15bc2b3f",
   "metadata": {},
   "source": [
    "## Choosing the Right Number of Dimensions\n",
    "Instead of Choosing the Number of Dimensions by mere Guess we can Set the Percentage of the Data that we want to Preserve from a scale between 0 to 1 in the **n_components** hyperparameter. Lets apply it on the MNIST Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b55b105-90ec-4a80-80f4-f241d0315e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/praddyumny/MachineLearning/Hands_On_Machine_Learning/venv/lib64/python3.12/site-packages/sklearn/datasets/_openml.py:1022: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 122.25525533, -316.23384391,  -51.13183087, ...,   34.71703473,\n",
       "         -14.22575676,   21.38272145],\n",
       "       [1010.49400346, -289.96362059,  576.1207452 , ...,   23.87884359,\n",
       "          -6.54283564,  -24.90277545],\n",
       "       [ -58.99594719,  393.69744499, -161.99818411, ...,   -5.36282742,\n",
       "          55.00020853,  -96.73397123],\n",
       "       ...,\n",
       "       [-271.50701323,  590.07850009,  341.36886918, ...,  -43.7571469 ,\n",
       "          35.78216024,   49.96612771],\n",
       "       [-310.22482291, -116.72715081,  635.71999693, ...,  -21.86345345,\n",
       "          20.40152778,  -42.68277473],\n",
       "       [1058.86212574,  -83.39253843,  731.34218396, ...,   41.22834049,\n",
       "         -20.05206663,  -49.92361814]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml(\"mnist_784\", version=1)\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "\n",
    "pca = PCA(n_components=0.95) # Preserve 95% Data of the Dataset\n",
    "X_reduced = pca.fit_transform(X)\n",
    "X_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a8c60cc-af28-4029-9773-917e9fcc510f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 154)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7a1571-c101-437d-bcba-140c30bab726",
   "metadata": {},
   "source": [
    "## PCA for Compression\n",
    "We can see that after applying PCA to MNIST it Preserves 95% Variance and the Dataset is now Less 20% BTW we can also recover the original dataset (however it it will not be the exact same some of the things can be different)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fce9c8b1-11dd-4531-b292-83f790b03dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_recovered = pca.inverse_transform(X_reduced)\n",
    "X_recovered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fae88ec-7213-4545-9fc0-455848a95170",
   "metadata": {},
   "source": [
    "$$\n",
    "X_{recovered} = X_{d-proj} . W_d^T\n",
    "$$\n",
    "**--------------------------------------------- Equation 8-3: PCA inverse transformation ---------------------------------------------**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9c7f44-c864-464f-92e1-0dcb3c1d7c34",
   "metadata": {},
   "source": [
    "## Randomized PCA\n",
    "If you set *svd_solver* hyperparameter to \"randomized\" Scikit-Learn uses Stochastic Algorithm called **Randomized PCA**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b60ef39-24dd-4805-9179-d4d80070a030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 154)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_pca = PCA(n_components=154, svd_solver=\"randomized\")\n",
    "X_reduced = rnd_pca.fit_transform(X)\n",
    "X_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0fcce0-ded8-452c-8566-87a120bcafc2",
   "metadata": {},
   "source": [
    "## Incremental PCA\n",
    "One problem with PCA is that they require whole dataset to fit in the memory but luckily we can avoid it by using **Incremental PCA(IPCA)** which splits the datasets in mini-batches and this is a good way to upgrade to online learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67ab5c00-c7ff-457f-b0a4-b36b835bc172",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/praddyumny/MachineLearning/Hands_On_Machine_Learning/venv/lib64/python3.12/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(70000, 154)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "n_batches = 100\n",
    "inc_pca = IncrementalPCA(n_components=154)\n",
    "for X_batch in np.array_split(X, n_batches):\n",
    "    inc_pca.partial_fit(X_batch)\n",
    "\n",
    "X_reduced = inc_pca.transform(X)\n",
    "X_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b948e4a-fb14-44ce-ba84-f252f2f225d2",
   "metadata": {},
   "source": [
    "# Kernel PCA\n",
    "In Chapter 5 we discussed about a Kernel Trick a methematical technique that implicitly maps instances into a higher dimensional space called feature space enabling non-linear classification and regression with Support Vector Machines(SVMs).\n",
    "It turns Out that the same trick can be applied to PCA makeing it possible to project complex non-linear projections for dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf735ef9-a689-4b2f-82f2-89a87b53edf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_swiss_roll\n",
    "X, color = make_swiss_roll()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ee9c825-0723-4ea1-90e7-ef97061fc176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "rbf_pca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=0.04)\n",
    "X_reduced = rbf_pca.fit_transform(X)\n",
    "X_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4844bc0c-5e45-4ee2-9e6e-0334d1352f1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
