{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc90b26f-d08e-426c-98dd-a454460b247a",
   "metadata": {},
   "source": [
    "# **Chapter 8 - Dimensionality Reduction**\n",
    "Many Machine Learning Problems Involve thousands or even hundreds of features to Train the Models which In turn gives birth to **the curse of dimensionality**(Explained Later in the Chapter) and as a solution to this Problem comes Dimensionality Reduction here are two of the advantages of using Dimensionality Reduction :-\n",
    "- Speed up the Process of Training.\n",
    "- Helpful in Data Visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2560910e-6e6c-43a7-a3ce-603e1f6e0d77",
   "metadata": {},
   "source": [
    "# The Curse of Dimensionality\n",
    "- Sometimes Our Machine Learning Model gets confused with the Noise of the Dataset.\n",
    "    - Most of the times due to high number of features.\n",
    "- High Numbers of features in a dataset results in slow speed of Training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8749b2-bb4f-4b6b-a8b5-c9a2093290c0",
   "metadata": {},
   "source": [
    "# Main Approaches of Dimensionality Reduction\n",
    "There are two main Approaches of Dimensionality Reduction in Machine Learning :-\n",
    "\n",
    "## Projection\n",
    "In Most Real World Problems the the Dataset is not Uniformly Distrubuted Throughout all Dimensions. Many Features are almost Constant.\n",
    "So in the Projection We Find out the Plane in which most of the data points lie and then Project the Whole Dataset on that specific Plane.\n",
    "However somtimes we cannot find a plane on the Dataset Like in the famous Swiss Roll Dataset in Which You have to unroll the dataset to get accurate representation.\n",
    "\n",
    "## Manifold Learning\n",
    "Manifold Learning is a Dimensionality Reduction Algorithm which Tries to find Familiar Shapes or Structures inside the Dataset and then Projects the Dataset Unto a Lower Dimensional Plane hence Reducing its Dimension.\n",
    "- i.e > Unrolling the Swiss Roll Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d77bc5-f59f-436b-8009-40324831a484",
   "metadata": {},
   "source": [
    "# PCA(Principal Component Analysis)\n",
    "- This is the Most Popular Dimensionality Redution Algorithm.\n",
    "- First It Identifies the Hyperplane which Lies closest to the Data.\n",
    "- Then It Projects the Data Onto that Hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42866d89-4ab5-4610-8a4e-77fc025e86b9",
   "metadata": {},
   "source": [
    "## Preserving the Variance\n",
    "- Before You Project the Data onto the Hyperplane You Need to Select the Right Hyperplane.\n",
    "- The Best Way to Select the Right Hyperplane is to choose the Hyperplane which Preserves the Maximum Variance.\n",
    "    - Because it will in turn preserve the Data Loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999be24b-9844-4673-b202-562727fe41ac",
   "metadata": {},
   "source": [
    "## Principal Component\n",
    "- PCA Identifies the axis that accounts the largest amount of Variance in the training Set.\n",
    "- The $i^{th}$ axis is also called the $i^{th}$ *principal component*.\n",
    "- We find the principal component for the dataset with help of a standard matrix factorization called **Singular Value Decomposition(SVD)** here is an implementation of this function with python code :-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6842db8-46db-44f8-9411-8b7e4270a867",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c33a4d7-c042-45d8-b17b-cd9bf3f3c5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_centered = X - X.mean(axis=0)\n",
    "U, s, Vt = np.linalg.svd(X_centered)\n",
    "c1 = Vt.T[:, 0]\n",
    "c2 = Vt.T[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60efb71-223b-4004-89a3-2d7dacc64667",
   "metadata": {},
   "source": [
    "**V** contains all of the Pricipal Components that we are looking for :-\n",
    "$$\n",
    "V = (c_1, c_2, ..., c_n)\n",
    "$$\n",
    "- here vectors from $c_1$ to $c_n$ are the Pricipal Components from which main Pricipal Componenet will be choosed on the basis of Preserving Variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95b0956-6a5c-426f-b56b-48f8cf097560",
   "metadata": {},
   "source": [
    "## Projecting Down to d Dimensions\n",
    "Now that we have Identified our Pricipal Component we can obtain the reduced data by Prjecting it onto the Pricipal component of our choice.\n",
    "- To Project the training set onto the hyperplane and obtain a reduced dataset $X_{d-proj}$ we have to compute a matrix multiplication.\n",
    "\n",
    "$$\n",
    "X_{d-proj} = XW_d\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "682c5469-a8bc-490a-ab87-bb2708fc9ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = Vt.T[:, :2]\n",
    "X2D = X_centered.dot(W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65036e53-534f-4883-b26e-07f9257de900",
   "metadata": {},
   "source": [
    "## Using Scikit-Learn\n",
    "Like all the other Techniques and other things *Scikit-Learn* also Provides an implementation of **PCA** :-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8843c81-94d9-4e9f-bbf2-12f5aafd073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X2D = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e558190-471d-4cb1-ab94-9693488007a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.81968748, 0.18031252])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2185ef-c0eb-4ee1-b7a4-eb63b6e2a319",
   "metadata": {},
   "source": [
    "This tells YOu that the VAriance in the first PC was 81% and in the Second PC it was 18% which means that we have Preserved 81% of Our Data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6272a09-4bf6-40da-80c5-9c5f15bc2b3f",
   "metadata": {},
   "source": [
    "## Choosing the Right Number of Dimensions\n",
    "Instead of Choosing the Number of Dimensions by mere Guess we can Set the Percentage of the Data that we want to Preserve from a scale between 0 to 1 in the **n_components** hyperparameter. Lets apply it on the MNIST Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b55b105-90ec-4a80-80f4-f241d0315e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/praddyumny/MachineLearning/Hands_On_Machine_Learning/venv/lib64/python3.12/site-packages/sklearn/datasets/_openml.py:1022: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 122.25525533, -316.23384391,  -51.13183087, ...,   34.71703473,\n",
       "         -14.22575676,   21.38272145],\n",
       "       [1010.49400346, -289.96362059,  576.1207452 , ...,   23.87884359,\n",
       "          -6.54283564,  -24.90277545],\n",
       "       [ -58.99594719,  393.69744499, -161.99818411, ...,   -5.36282742,\n",
       "          55.00020853,  -96.73397123],\n",
       "       ...,\n",
       "       [-271.50701323,  590.07850009,  341.36886918, ...,  -43.7571469 ,\n",
       "          35.78216024,   49.96612771],\n",
       "       [-310.22482291, -116.72715081,  635.71999693, ...,  -21.86345345,\n",
       "          20.40152778,  -42.68277473],\n",
       "       [1058.86212574,  -83.39253843,  731.34218396, ...,   41.22834049,\n",
       "         -20.05206663,  -49.92361814]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml(\"mnist_784\", version=1)\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "\n",
    "pca = PCA(n_components=0.95) # Preserve 95% Data of the Dataset\n",
    "X_reduced = pca.fit_transform(X)\n",
    "X_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a8c60cc-af28-4029-9773-917e9fcc510f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 154)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7a1571-c101-437d-bcba-140c30bab726",
   "metadata": {},
   "source": [
    "## PCA for Compression\n",
    "We can see that after applying PCA to MNIST it Preserves 95% Variance and the Dataset is now Less 20% BTW we can also recover the original dataset (however it it will not be the exact same some of the things can be different)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fce9c8b1-11dd-4531-b292-83f790b03dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_recovered = pca.inverse_transform(X_reduced)\n",
    "X_recovered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fae88ec-7213-4545-9fc0-455848a95170",
   "metadata": {},
   "source": [
    "$$\n",
    "X_{recovered} = X_{d-proj} . W_d^T\n",
    "$$\n",
    "**--------------------------------------------- Equation 8-3: PCA inverse transformation ---------------------------------------------**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9c7f44-c864-464f-92e1-0dcb3c1d7c34",
   "metadata": {},
   "source": [
    "## Randomized PCA\n",
    "If you set *svd_solver* hyperparameter to \"randomized\" Scikit-Learn uses Stochastic Algorithm called **Randomized PCA**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b60ef39-24dd-4805-9179-d4d80070a030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 154)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_pca = PCA(n_components=154, svd_solver=\"randomized\")\n",
    "X_reduced = rnd_pca.fit_transform(X)\n",
    "X_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0fcce0-ded8-452c-8566-87a120bcafc2",
   "metadata": {},
   "source": [
    "## Incremental PCA\n",
    "One problem with PCA is that they require whole dataset to fit in the memory but luckily we can avoid it by using **Incremental PCA(IPCA)** which splits the datasets in mini-batches and this is a good way to upgrade to online learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67ab5c00-c7ff-457f-b0a4-b36b835bc172",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/praddyumny/MachineLearning/Hands_On_Machine_Learning/venv/lib64/python3.12/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(70000, 154)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "n_batches = 100\n",
    "inc_pca = IncrementalPCA(n_components=154)\n",
    "for X_batch in np.array_split(X, n_batches):\n",
    "    inc_pca.partial_fit(X_batch)\n",
    "\n",
    "X_reduced = inc_pca.transform(X)\n",
    "X_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b948e4a-fb14-44ce-ba84-f252f2f225d2",
   "metadata": {},
   "source": [
    "# Kernel PCA\n",
    "In Chapter 5 we discussed about a Kernel Trick a methematical technique that implicitly maps instances into a higher dimensional space called feature space enabling non-linear classification and regression with Support Vector Machines(SVMs).\n",
    "It turns Out that the same trick can be applied to PCA makeing it possible to project complex non-linear projections for dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf735ef9-a689-4b2f-82f2-89a87b53edf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_swiss_roll\n",
    "X, color = make_swiss_roll()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ee9c825-0723-4ea1-90e7-ef97061fc176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "rbf_pca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=0.04)\n",
    "X_reduced = rbf_pca.fit_transform(X)\n",
    "X_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879741ed-bea4-4c03-8fd5-bcebfde33537",
   "metadata": {},
   "source": [
    "## Selecting and Tuning Hyperparameters\n",
    "We know that Kernel PCA is an Unsupervised Learning Algorithm so there isn't any obvious performance measure for it but it is kind of a preparation step for a supervised learning task so that means we can use GridSearchCV to choose the kernel which leads to the best performance on that particular task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75b68ad8-a69e-434e-b847-861feed014c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b32a666d-3f3a-47c1-8238-f154eae83653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[(&#x27;kpca&#x27;, KernelPCA(n_components=2)),\n",
       "                                       (&#x27;log_reg&#x27;, LogisticRegression())]),\n",
       "             param_grid=[{&#x27;kpca__gamma&#x27;: array([0.03      , 0.03222222, 0.03444444, 0.03666667, 0.03888889,\n",
       "       0.04111111, 0.04333333, 0.04555556, 0.04777778, 0.05      ]),\n",
       "                          &#x27;kpca__kernel&#x27;: [&#x27;rbf&#x27;, &#x27;sigmoid&#x27;]}])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[(&#x27;kpca&#x27;, KernelPCA(n_components=2)),\n",
       "                                       (&#x27;log_reg&#x27;, LogisticRegression())]),\n",
       "             param_grid=[{&#x27;kpca__gamma&#x27;: array([0.03      , 0.03222222, 0.03444444, 0.03666667, 0.03888889,\n",
       "       0.04111111, 0.04333333, 0.04555556, 0.04777778, 0.05      ]),\n",
       "                          &#x27;kpca__kernel&#x27;: [&#x27;rbf&#x27;, &#x27;sigmoid&#x27;]}])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;kpca&#x27;, KernelPCA(n_components=2)),\n",
       "                (&#x27;log_reg&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KernelPCA</label><div class=\"sk-toggleable__content\"><pre>KernelPCA(n_components=2)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[('kpca', KernelPCA(n_components=2)),\n",
       "                                       ('log_reg', LogisticRegression())]),\n",
       "             param_grid=[{'kpca__gamma': array([0.03      , 0.03222222, 0.03444444, 0.03666667, 0.03888889,\n",
       "       0.04111111, 0.04333333, 0.04555556, 0.04777778, 0.05      ]),\n",
       "                          'kpca__kernel': ['rbf', 'sigmoid']}])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "clf = Pipeline([\n",
    "    (\"kpca\", KernelPCA(n_components=2)),\n",
    "    (\"log_reg\", LogisticRegression())\n",
    "])\n",
    "\n",
    "param_grid = [{\n",
    "    \"kpca__gamma\": np.linspace(0.03, 0.05, 10),\n",
    "    \"kpca__kernel\": [\"rbf\", \"sigmoid\"]\n",
    "}]\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=3)\n",
    "grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30d8e3fc-51c3-4cf8-8b94-58c88fbabafe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kpca__gamma': 0.03, 'kpca__kernel': 'rbf'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e6045e-1af9-4e17-8323-198c4026226f",
   "metadata": {},
   "source": [
    "# LLE - Locally Linear Embedding\n",
    "This is a Dimensionality Reduction Algorithm which takes a very different Path in Comparison with PCA and Many others so Basically what it does is that it finds the relationship between every instance and its closest instances then it reduces the dimensions of it and then it tries to re-establish the relationship between them here is an implementation from scikit-learn :-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7a97b70-6d88-420d-9b14-3fd90bad432b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)\n",
    "X_reduced = lle.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf73ce03-db83-499a-a698-389f29c75b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0240951 ,  0.0917334 ],\n",
       "       [ 0.15128302, -0.12505169],\n",
       "       [ 0.1697423 , -0.17336364],\n",
       "       [-0.16974231, -0.17336364],\n",
       "       [-0.06184254,  0.06126423],\n",
       "       [-0.02881438,  0.08971436],\n",
       "       [-0.17425115, -0.18522388],\n",
       "       [ 0.04768922,  0.07656466],\n",
       "       [-0.183022  , -0.20834723],\n",
       "       [-0.11841504, -0.04321621],\n",
       "       [ 0.10428083, -0.011587  ],\n",
       "       [ 0.18726189, -0.21952952],\n",
       "       [-0.09013917,  0.01681707],\n",
       "       [ 0.06655963,  0.05511734],\n",
       "       [-0.01465651,  0.0943087 ],\n",
       "       [ 0.1372327 , -0.08904099],\n",
       "       [-0.01937563,  0.09326113],\n",
       "       [ 0.0571251 ,  0.06688794],\n",
       "       [-0.01829187,  0.08832706],\n",
       "       [-0.0276747 ,  0.08233493],\n",
       "       [-0.09485365,  0.00775414],\n",
       "       [-0.0099394 ,  0.09488361],\n",
       "       [-0.14192441, -0.10093727],\n",
       "       [ 0.12782966, -0.06570539],\n",
       "       [-0.11370462, -0.03236172],\n",
       "       [ 0.08070863,  0.03360242],\n",
       "       [ 0.01829186,  0.08832706],\n",
       "       [-0.03653287,  0.07584864],\n",
       "       [-0.04162802,  0.06925885],\n",
       "       [-0.00521813,  0.09493374],\n",
       "       [ 0.09013916,  0.01681707],\n",
       "       [ 0.08542413,  0.02543971],\n",
       "       [ 0.1419244 , -0.10093727],\n",
       "       [-0.15128303, -0.12505169],\n",
       "       [-0.13723271, -0.08904099],\n",
       "       [ 0.0759927 ,  0.04127685],\n",
       "       [-0.12312341, -0.05434004],\n",
       "       [-0.12782967, -0.06570539],\n",
       "       [ 0.09485364,  0.00775414],\n",
       "       [ 0.00420861,  0.09372354],\n",
       "       [-0.10428084, -0.011587  ],\n",
       "       [ 0.09956757, -0.00172275],\n",
       "       [-0.07127637,  0.04845505],\n",
       "       [ 0.10899324, -0.02180922],\n",
       "       [ 0.04162801,  0.06925885],\n",
       "       [-0.06655963,  0.05511734],\n",
       "       [ 0.16057084, -0.14929489],\n",
       "       [ 0.11370462, -0.03236172],\n",
       "       [-0.03229253,  0.07879806],\n",
       "       [-0.1872619 , -0.21952952],\n",
       "       [-0.05712511,  0.06688794],\n",
       "       [ 0.02881437,  0.08971436],\n",
       "       [-0.03825223,  0.08416099],\n",
       "       [-0.17868154, -0.19689906],\n",
       "       [-0.01364472,  0.09046247],\n",
       "       [ 0.17425114, -0.18522388],\n",
       "       [ 0.17868153, -0.19689906],\n",
       "       [-0.04297087,  0.08062181],\n",
       "       [-0.05240732,  0.07198761],\n",
       "       [ 0.02409509,  0.0917334 ],\n",
       "       [ 0.11841503, -0.04321621],\n",
       "       [-0.10899324, -0.02180922],\n",
       "       [ 0.05240731,  0.07198761],\n",
       "       [ 0.02300132,  0.08554125],\n",
       "       [ 0.02767469,  0.08233493],\n",
       "       [-0.1466101 , -0.11295883],\n",
       "       [ 0.0146565 ,  0.0943087 ],\n",
       "       [ 0.03653286,  0.07584864],\n",
       "       [ 0.00521812,  0.09493374],\n",
       "       [ 0.00891865,  0.0923686 ],\n",
       "       [ 0.00993939,  0.09488361],\n",
       "       [ 0.06184253,  0.06126423],\n",
       "       [-0.00420861,  0.09372354],\n",
       "       [-0.00891865,  0.0923686 ],\n",
       "       [-0.13253316, -0.07728288],\n",
       "       [ 0.03353322,  0.08718773],\n",
       "       [-0.02300133,  0.08554125],\n",
       "       [ 0.0005032 ,  0.09457601],\n",
       "       [ 0.14661009, -0.11295883],\n",
       "       [ 0.13253315, -0.07728288],\n",
       "       [-0.16516915, -0.16136247],\n",
       "       [-0.00050321,  0.09457601],\n",
       "       [ 0.12312341, -0.05434004],\n",
       "       [-0.03353322,  0.08718773],\n",
       "       [-0.07599271,  0.04127685],\n",
       "       [ 0.15593895, -0.13717724],\n",
       "       [ 0.03825222,  0.08416099],\n",
       "       [ 0.04297087,  0.08062181],\n",
       "       [ 0.16516914, -0.16136247],\n",
       "       [-0.16057085, -0.14929489],\n",
       "       [-0.04768923,  0.07656466],\n",
       "       [ 0.07127636,  0.04845505],\n",
       "       [-0.08070864,  0.03360242],\n",
       "       [-0.15593896, -0.13717724],\n",
       "       [ 0.01364472,  0.09046247],\n",
       "       [ 0.18302199, -0.20834723],\n",
       "       [-0.08542414,  0.02543971],\n",
       "       [ 0.03229252,  0.07879806],\n",
       "       [ 0.01937562,  0.09326113],\n",
       "       [-0.09956758, -0.00172275]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa8a6e4-18fc-45c4-b9b3-c58ef2348f40",
   "metadata": {},
   "source": [
    "This algotithm particularly works better on the datasets like swiss roll dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c06f63-415d-40b8-bdc6-d0f0987d8b4d",
   "metadata": {},
   "source": [
    "# Other Dimensionlaity Reduction Techniques\n",
    "There are So many Dimensionality Reduction Techniques in Scikit-Learn Here are some of the most important ones :-\n",
    "## Random Projections\n",
    "As its name suggests it just Projects the Dataset Randomly to a Lower Dimensional Plane, Well! It sounds crazy but there is a very high possibility that it preserves the distances very well (Proved by **William B. Johnson** and **Joran Lindenstrauss**).\n",
    "## Multi-Dimensional Scaling (MDS)\n",
    "Reduces dimensionality while trying to preserve the distances between the instances.\n",
    "## Isomap\n",
    "Measures Distance between Closest Instances then it tries to Preserve the Distances between them while reducing Dimensions.\n",
    "## t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "Tries to Reduce the Dimensions while trying to Close the Distance Between Similar Objects and Increase the Distance between Disssimilar Objects (It is mainly used for Visualization Purposes).\n",
    "## Linear Discriminant Analysis (LDA)\n",
    "It is a Classification Algorithm so it learns the most discriminative axes then it can be used to create a Hyperplane on which the dataset is yet to be projected It is a good warm up algorithm it is often Good to run it on a dataset before training other Classifier Like SVM Classfier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4e1745-75c8-4b5b-b66a-0b9eae817864",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
